{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmVGr78Wipda"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Clicked Ads Dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1M89_zZxrj-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate analysis on age column, daily internet use, and daily time spent on"
      ],
      "metadata": {
        "id": "Ytg7VppUmGWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Uni = ['Age','Daily Internet Usage','Daily Time Spent on Site']"
      ],
      "metadata": {
        "id": "Ip6soo7H1iZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Uni)):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    sns.countplot(x=df[Uni[i]], data=df, color='green')"
      ],
      "metadata": {
        "id": "jAqW5nml1fON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(data=df, x=\"Age\", hue=\"Clicked on Ad\", kde=True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a3TE_A7ow1Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(data=df, x=\"Daily Internet Usage\", hue=\"Clicked on Ad\", kde=True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P2XldV6oxS2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(data=df, x=\"Daily Time Spent on Site\", hue=\"Clicked on Ad\", kde=True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C4eeZ3Q4xgmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bivariate analysis on variables that have been studied in univariate analysis"
      ],
      "metadata": {
        "id": "xJ5KYFcpx4iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df, x='Age', y='Daily Internet Usage', hue='Clicked on Ad')\n",
        "plt.title('Age vs Daily Internet Usage')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CB_COuNaMGSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df, x='Age', y='Daily Time Spent on Site', hue='Clicked on Ad')\n",
        "plt.title('Age vs Daily Time Spent on Site')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qjOnLRuuL8Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df, x='Daily Internet Usage', y='Daily Time Spent on Site', hue='Clicked on Ad')\n",
        "plt.title('Daily Internet Usage vs Daily Time Spent on Site')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vxlbr_yQMKDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlate between columns and perform multivariate analysis"
      ],
      "metadata": {
        "id": "MFsAU9880lUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "id": "cJtkRlWL0lAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n",
        "# save heatmap as .png file\n",
        "# dpi - sets the resolution of the saved image in dots/inches\n",
        "# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\n",
        "plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n"
      ],
      "metadata": {
        "id": "1QsNM1Go1CuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the data type\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')"
      ],
      "metadata": {
        "id": "cTJhjkWLLwG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename column\n",
        "df = df.rename(columns = {'Male' : 'Gender'} )\n"
      ],
      "metadata": {
        "id": "8bO3wG9uPgDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the dataset from missing values ​​and duplicated values\n"
      ],
      "metadata": {
        "id": "p-dS_Ej09lqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "8LGSHB2QDdFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df.dropna()"
      ],
      "metadata": {
        "id": "mLg8yiDMDj5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.isnull().sum()"
      ],
      "metadata": {
        "id": "a3a2w8LjD_qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicates of all columns\n",
        "\n",
        "new_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "6DHg5Xd4GU2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature encoding on the dataset\n"
      ],
      "metadata": {
        "id": "bgP8UfyqGh9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.info()"
      ],
      "metadata": {
        "id": "c0P4-BP6HEkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Categorical = ['Gender','Timestamp','Clicked on Ad', 'city' , 'province', 'category']\n",
        "Numerical = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage']"
      ],
      "metadata": {
        "id": "FPZ7yJb2G_qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_enc = new_df.copy()\n",
        "df_encoded = pd.get_dummies(df_enc, columns=Categorical)\n",
        "\n",
        "\n",
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "KRVgZAejy9kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into features and targets"
      ],
      "metadata": {
        "id": "WBzn9XXlIPkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_encoded.drop(columns=['Clicked on Ad_Yes'])\n",
        "y = df_encoded['Clicked on Ad_Yes']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Total training data:\", X_train.shape[0])\n",
        "print(\"Number of test data:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "MGtk3v8lIOt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the column related to time"
      ],
      "metadata": {
        "id": "wjP-vuEBJUiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# year extraction\n",
        "df_encoded['Tahun'] = df_enc['Timestamp'].dt.year\n",
        "\n",
        "# Month extraction\n",
        "df_encoded['Bulan'] = df_enc['Timestamp'].dt.month\n",
        "\n",
        "# Week extraction\n",
        "df_encoded['Pekan'] = df_enc['Timestamp'].dt.isocalendar().week\n",
        "\n",
        "# Day extraction\n",
        "df_encoded['Hari'] = df_enc['Timestamp'].dt.day\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "ESlqLLpmJgbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Share data separately which will later be used as training data and test data"
      ],
      "metadata": {
        "id": "VZFk1qHk_kZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_encoded.drop(columns=['Clicked on Ad_Yes'])\n",
        "y = df_encoded['Clicked on Ad_Yes']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Total training data:\", X_train.shape[0])\n",
        "print(\"Number of test data:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "m5OBRGCW_ZUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model"
      ],
      "metadata": {
        "id": "qv_ZtloKKg98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before Normalization"
      ],
      "metadata": {
        "id": "UrDYEhJjPohl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "l2Jh1IruJKg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(C = 1000.0, random_state = 0)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "print('Accuracy: ')\n",
        "print('{}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report: ')\n",
        "print('{}'.format(classification_report(y_test, y_pred)))\n",
        "print('Confusion Matrix')\n",
        "print('{}'.format(confusion_matrix(y_test, y_pred)))\n",
        "print('Cohen kappa score: ')\n",
        "print('{}'.format(cohen_kappa_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "ZGAqX493_tnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "OgkL4sqoJLkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(criterion = 'entropy'\n",
        "                                , n_estimators = 10\n",
        "                                , random_state = 1\n",
        "                                , n_jobs = 1)\n",
        "rfc.fit(X_train, y_train)\n",
        "y_pred = rfc.predict(X_test)\n",
        "print('Accuracy: ')\n",
        "print('{}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report: ')\n",
        "print('{}'.format(classification_report(y_test, y_pred)))\n",
        "print('Confusion Matrix')\n",
        "print('{}'.format(confusion_matrix(y_test, y_pred)))\n",
        "print('Cohen kappa score: ')\n",
        "print('{}'.format(cohen_kappa_score(y_test, y_pred)))\n",
        "\n"
      ],
      "metadata": {
        "id": "q76z0TABJObw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "2EbRC4pmJO8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC(kernel = 'rbf', C = 1.0, random_state = 0)\n",
        "svc.fit(X_train, y_train)\n",
        "y_pred = svc.predict(X_test)\n",
        "print('Accuracy: ')\n",
        "print('{}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report: ')\n",
        "print('{}'.format(classification_report(y_test, y_pred)))\n",
        "print('Confusion Matrix')\n",
        "print('{}'.format(confusion_matrix(y_test, y_pred)))\n",
        "print('Cohen kappa score: ')\n",
        "print('{}'.format(cohen_kappa_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "58SA0oMTJR3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Normalization"
      ],
      "metadata": {
        "id": "WvTe7PavUqif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalization of training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Normalization of test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"X_train without standardising features\")\n",
        "print(\"--------------------------------------\")\n",
        "print(X_train.loc[1:4, ])  # Using loc for label based indexes\n",
        "\n",
        "print(\"\")\n",
        "print(\"X_train standardising features\")\n",
        "print(\"--------------------------------------\")\n",
        "print(X_train_scaled[1:5, :])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GY9LLH6l-7Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler object\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Features to be standardized\n",
        "\n",
        "# Standardize the training data\n",
        "X_train_standard = sc.fit_transform(X_train_scaled)\n",
        "\n",
        "# Perform standardization on test data\n",
        "X_test_standard = sc.transform(X_test)\n",
        "\n",
        "print(\"X_train without standardising features\")\n",
        "print(\"--------------------------------------\")\n",
        "print(X_train.loc[1:4, ])  # Using loc for label based indexes\n",
        "print(\"\")\n",
        "print(\"X_train standardising features\")\n",
        "print(\"--------------------------------------\")\n",
        "print(X_train_standard[1:5, :])\n"
      ],
      "metadata": {
        "id": "j8kbjaYq17fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example of training data\n",
        "example_train_data = np.array([[10], [20], [30]])\n",
        "\n",
        "# Initialize the scaler object\n",
        "example_sc = StandardScaler()\n",
        "\n",
        "# Perform mean and standard deviation calculations on training data\n",
        "example_sc.fit(example_train_data)\n",
        "\n",
        "# Perform transformation on training data\n",
        "example_train_data_scaled = example_sc.transform(example_train_data)\n",
        "\n",
        "print(\"Example train data\")\n",
        "print(\"------------------\")\n",
        "print(example_train_data)\n",
        "print(\"Example train data scaled\")\n",
        "print(\"------------------------\")\n",
        "print(example_train_data_scaled)\n",
        "print(\"----------------------------------------------\")\n",
        "print(\"\")\n",
        "print(\"What would happen if, instead of scaling the test dataset with the training scaling parameters, we scaled\")\n",
        "print(\"with the test scaling parameters?\")\n",
        "\n",
        "# Sample test data\n",
        "example_test_data = np.array([[5], [6], [7]])\n",
        "\n",
        "# Perform transformation on test data\n",
        "example_test_data_scaled = example_sc.transform(example_test_data)\n",
        "\n",
        "print(\"Example test data\")\n",
        "print(\"-----------------\")\n",
        "print(example_test_data)\n",
        "print(\"Example test data scaled\")\n",
        "print(\"-----------------------\")\n",
        "print(example_test_data_scaled)\n"
      ],
      "metadata": {
        "id": "uAZy_y9B9iFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "_neWZp5MVD2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(C = 1000.0, random_state = 0 )\n",
        "lr.fit(X_train_standard, y_train)\n",
        "Y_pred_Logit = lr.predict(X_test_standard)\n",
        "print('Accuracy: ')\n",
        "print('{}'.format(accuracy_score(y_test, Y_pred_Logit)))\n",
        "print('Classification report: ')\n",
        "print('{}'.format(classification_report(y_test, Y_pred_Logit)))\n",
        "print('Confusion Matrix')\n",
        "print('{}'.format(confusion_matrix(y_test, Y_pred_Logit)))\n",
        "print('Cohen kappa score: ')\n",
        "print('{}'.format(cohen_kappa_score(y_test, Y_pred_Logit)))\n",
        "\n"
      ],
      "metadata": {
        "id": "KWTtyoW7VDR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "nyP5Bj9MVQEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(criterion = 'entropy'\n",
        "                                , n_estimators = 10\n",
        "                                , random_state = 1\n",
        "                                , n_jobs = 1)\n",
        "rfc.fit(X_train_standard, y_train)\n",
        "Y_pred_RF = rfc.predict(X_test_standard)\n",
        "print('Accuracy: ')\n",
        "print('{}'.format(accuracy_score(y_test, Y_pred_RF)))\n",
        "print('Classification report: ')\n",
        "print('{}'.format(classification_report(y_test, Y_pred_RF)))\n",
        "print('Confusion Matrix')\n",
        "print('{}'.format(confusion_matrix(y_test, Y_pred_RF)))\n",
        "print('Cohen kappa score: ')\n",
        "print('{}'.format(cohen_kappa_score(y_test, Y_pred_RF)))"
      ],
      "metadata": {
        "id": "aRwccEZyVQNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "cDWazlY5VWwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel = 'linear', C = 1.0, random_state = 0)\n",
        "svm.fit(X_train_standard, y_train)\n",
        "Y_pred_SVM = svm.predict(X_test_standard)\n",
        "print('Accuracy: ')\n",
        "print('{}'.format(accuracy_score(y_test, Y_pred_SVM)))\n",
        "print('Classification report: ')\n",
        "print('{}'.format(classification_report(y_test, Y_pred_SVM)))\n",
        "print('Confusion Matrix')\n",
        "print('{}'.format(confusion_matrix(y_test, Y_pred_SVM)))\n",
        "print('Cohen kappa score: ')\n",
        "print('{}'.format(cohen_kappa_score(y_test, Y_pred_SVM)))"
      ],
      "metadata": {
        "id": "J6ksaXpHVYIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate the model using the confusion matrix and show features\n",
        "important from the model results"
      ],
      "metadata": {
        "id": "jbt2BVjWOWz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "Y2ZmlmpIpiOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "lr = LogisticRegression(C = 1000.0, random_state = 0 )\n",
        "lr.fit(X_train_standard, y_train)\n",
        "Y_pred_Logit = lr.predict(X_test_standard)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predictions\")\n",
        "plt.ylabel(\"Original value\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YbMXONIhpqHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "pdlUv93-pfs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "rfc = RandomForestClassifier(criterion = 'entropy'\n",
        "                                , n_estimators = 10\n",
        "                                , random_state = 1\n",
        "                                , n_jobs = 1)\n",
        "rfc.fit(X_train_standard, y_train)\n",
        "Y_pred_RF = rfc.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predictions\")\n",
        "plt.ylabel(\"Original value\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "UpUPFTwBOhgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_feature_importance(rfc):\n",
        "    feat_importances = pd.Series(rfc.feature_importances_, index=X.columns)\n",
        "    ax = feat_importances.nlargest(25).plot(kind= 'barh', figsize=(10, 8))\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "\n",
        "    plt.xlabel('score')\n",
        "    plt.ylabel('feature')\n",
        "    plt.title('feature importance score')\n",
        "\n",
        "show_feature_importance(rfc)"
      ],
      "metadata": {
        "id": "9jBnYr3NpG_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "HzBBB0YPrH8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "svm = SVC(kernel = 'linear', C = 1.0, random_state = 0)\n",
        "svm.fit(X_train_standard, y_train)\n",
        "Y_pred_SVM = svm.predict(X_test_standard)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predictions\")\n",
        "plt.ylabel(\"Original value\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "500TnWEBrNuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}